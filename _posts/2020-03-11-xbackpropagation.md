---  
title:  "순전파와 역전파 공부"  
  
categories:  
 - Deep learning  
tags:  
 - Study, Deep learning
 
---

# 순전파와 역전파
### 목차

-  Step 1. 인공 신경망의 이해
-  Step 2. 순전파(Forward Propagation)
-  Step 3. 역전파(Backward Propagation)

해당 게시물은 참고자료를 참고하거나 변형하여 작성하였습니다.

## Step 1. 인공신경망의 이해

순전파와 역전파를 이해하기 위해 사용될 인공 신경망을 예시로 소개한다. 해당 인공 신경망은 은닉층, 출력층의 모든 노드는 **활성화 함수** 로 시그모이드 함수를 사용한다. 

활성화 함수는 은닉층과 출력층의 노드에서 출력값을 결정하는 함수를 뜻하는데 앞서 공부했던 계단 함수, 시그모이드 함수 등이 존재한다. 활성화 함수에 대한 소개는 다음 포스팅때 더 자세히 공부하도록 하자.


![](https://wikidocs.net/images/page/37406/nn1_final.PNG)

해당 그림이 우리가 사용할 인공신경망의 모습이다. 
- $x$ : 입력값
- $W$ : 가중치
- $z$ : 이전 입력값과 가중치의 선형 결합($\sum xW$) = 활성화 함수의 입력값
-  $h$, $o$ : $z$ 값이 활성화 함수(시그모이드 함수)를 지난 후의 값 = 각 노드의 출력값

해당 인공 신경망은 편향(bias)는 고려하지 않음

###  과정

1. 순전파 과정을 통해 예측값과 실측값 오차 계산
2. 역전파 과정에서 경사 하강법(gradient descent)를 이용하여 가중치 업데이트 

## Step 2. 순전파(Foward Propagation)

![](https://wikidocs.net/images/page/37406/nn2_final_final.PNG)

파란색 숫자는 입력값을 의미하며, 빨간색 숫자는 가중치 값을 의미한다. 이제 해당 값들을 이용하여 순전파를 진행해보자.

순전파는 입력층에서 은닉층, 은닉층에서 출력층으로 최종 출력되는 과정을 말한다.

 
 -- 1) 입력층에서 은닉층 계산
 
$z_{1}=W_{1}x_{1} + W_{2}x_{2}=0.3 \text{×} 0.1 + 0.25 \text{×} 0.2= 0.08$

$z_{2}=W_{3}x_{1} + W_{4}x_{2}=0.4 \text{×} 0.1 + 0.35 \text{×} 0.2= 0.11$

--  2) 은닉층에서 활성화 함수 적용

$h_{1}=sigmoid(z_{1}) = 0.51998934$

$h_{2}=sigmoid(z_{2}) = 0.52747230$

-- 3) 은닉층에서 출력층 계산

$z_{3}=W_{5}h_{1}+W_{6}h_{2} = 0.45 \text{×} h_{1} + 0.4 \text{×} h_{2} = 0.44498412$
$z_{4}=W_{7}h_{1}+W_{8}h_{2} = 0.7 \text{×} h_{1} + 0.6 \text{×} h_{2} = 0.68047592$

--4) 최종 출력값(예측값)

$o_{1}=sigmoid(z_{3})=0.60944600$

$o_{2}=sigmoid(z_{4})=0.66384491$

--5) 오차 계산

예측값과 실제값의 오차를 계산하기 위해 손실함수 평균 제곱 오차(MSE) 사용을 한다. $target$은 실제값, $output$은 예측값을 의미한다.

$E_{o1}=\frac{1}{2}(target_{o1}-output_{o1})^{2}=0.02193381$

$E_{o2}=\frac{1}{2}(target_{o2}-output_{o2})^{2}=0.00203809$

전체 오차

$E_{total}=E_{o1}+E_{o2}=0.02397190$

## Step 3. 역전파 
순전파가 입력층에서 출력층으로 향했다면, 역전파는 출력층에서 입력층으로 향하면서 가중치를 업데이트한다. 

출력층과 은닉층 사이의 가중치를 업데이트하는 단계를 역전파 1단계, 그리고 은닉층과 입력층 사이의 가중치를 업데이트하는 단계를 역전파 2단계라고 하겠다.

### 1) 역전파 1단계

![](https://wikidocs.net/images/page/37406/nn3_final.PNG)

역전파 1단계에서 업데이트 할 가중치는 $W_5, W_6, W_7, W_8$ 총 네 개가 존재한다. 업데이트 과정은 모두 동일하니 $W_5$부터 계산해보자.

경사 하강법(gradient descent)를 계산하여 가중치 $W_5$
업데이트 해주기 위해서는 $\frac{∂E_{total}}{∂W_{5}}$를 계산해야한다.

 $\frac{∂E_{total}}{∂W_{5}}$를 계산하기 위해서는 앞선 포스팅에서 배웠던 연쇄 법칙(chain rule)을 사용하여 이와 같이 계산할 수 있다.

$$\frac{∂E_{total}}{∂W_{5}} = \frac{∂E_{total}}{∂o_{1}} \text{×} \frac{∂o_{1}}{∂z_{3}} \text{×} \frac{∂z_{3}}{∂W_{5}}$$

위의 식을 각 항에 대해서 순차적으로 계산해보자. 우선 첫번째 항부터 계산하기 전에 $E_{total}$부터 떠올려보자. 식은 아래와 같이 표현이 가능하다.

$E_{total}=\frac{1}{2}(target_{o1}-output_{o1})^{2} + \frac{1}{2}(target_{o2}-output_{o2})^{2}$

이에 $\frac{∂E_{total}}{∂o_{1}}$를 계산하면 다음과 같다.

$$\frac{∂E_{total}}{∂o_{1}}=2 \text{×} \frac{1}{2}(target_{o1}-output_{o1})^{2-1} \text{×} (-1) + 0$$

$$\frac{∂E_{total}}{∂o_{1}}=-(target_{o1}-output_{o1})=-(0.4-0.60944600)=0.20944600$$

자 이제 두번째 항을 계산해보자. $o_1$이라는 값은 시그모이드의 출력값이다. 참고로 시그모이드의 미분 공식은 $f(x) * (1-f(x))$이다. 따라서 두번째 항의 미분 결과는 다음과 같다.

$$\frac{∂o_{1}}{∂z_{3}}=o_{1}\text{×}(1-o_{1})=0.60944600(1-0.60944600)=0.23802157$$

마지막으로 세번째 항은 $h_1$과 동일하다.
$$\frac{∂z_{3}}{∂W_{5}}=h_{1}=0.51998934$$

이제 각각의 모든 항을 구했으니 이 값을 전부 곱해보자.

$$\frac{∂E_{total}}{∂W_{5}} = 0.20944600 \text{×} 0.23802157 \text{×} 0.51998934 = 0.02592286$$

이제 앞서 배웠던 경사 하강법을 통해 가중치를 업데이트를 할거다. 경사 하강법의 공식은 아래와 같다.

$$W := W - α\frac{∂}{∂W}Loss(W)$$

여기서 학습률 $α$는 0.5로 가정하자.

$$W_{5}^{+}=W_{5}-α\frac{∂E_{total}}{∂W_{5}}=0.45- 0.5 \text{×} 0.02592286=0.43703857$$

이와 같은 원리로 나머지 가중치들도 업데이트가 가능하다.

$$\frac{∂E_{total}}{∂W_{6}} = \frac{∂E_{total}}{∂o_{1}} \text{×} \frac{∂o_{1}}{∂z_{3}} \text{×} \frac{∂z_{3}}{∂W_{6}} → W_{6}^{+}=0.38685205$$$$\frac{∂E_{total}}{∂W_{7}} = \frac{∂E_{total}}{∂o_{2}} \text{×} \frac{∂o_{2}}{∂z_{4}} \text{×} \frac{∂z_{4}}{∂W_{7}} → W_{7}^{+}=0.69629578$$$$\frac{∂E_{total}}{∂W_{8}} = \frac{∂E_{total}}{∂o_{2}} \text{×} \frac{∂o_{2}}{∂z_{4}} \text{×} \frac{∂z_{4}}{∂W_{8}} → W_{8}^{+}=0.59624247$$

### 2) 역전파 2단계

![](https://wikidocs.net/images/page/37406/nn4.PNG)

1단계를 완료했으니, 이제 입력층 방향으로 이동하며 다시 계산을 이어간다. 현재 인공 신경망은 은닉층이 1개밖에 없음으로 이번이 마지막 단계이지만, 은닉층이 더 많은 경우라면 입력층 방향으로 한 단계씩 계속하여 계산해야한다.

이번 단계에서 계산할 가중치는 $W_1, W_2, W_3, W_4$이다. 원리 자체는 위와 동일하므로 계산과정은 생략하겠다. 

최종 결과 아래와 같다.
$$\frac{∂E_{total}}{∂W_{1}} = \frac{∂E_{total}}{∂h_{1}} \text{×} \frac{∂h_{1}}{∂z_{1}} \text{×} \frac{∂z_{1}}{∂W_{1}} → W_{1}^{+} = 0.29959556$$$$\frac{∂E_{total}}{∂W_{2}} = \frac{∂E_{total}}{∂h_{1}} \text{×} \frac{∂h_{1}}{∂z_{1}} \text{×} \frac{∂z_{1}}{∂W_{2}}  → W_{2}^{+}=0.24919112$$$$\frac{∂E_{total}}{∂W_{3}} = \frac{∂E_{total}}{∂h_{2}} \text{×} \frac{∂h_{2}}{∂z_{2}} \text{×} \frac{∂z_{2}}{∂W_{3}}  → W_{3}^{+}=0.39964496$$$$\frac{∂E_{total}}{∂W_{4}} = \frac{∂E_{total}}{∂h_{2}} \text{×} \frac{∂h_{2}}{∂z_{2}} \text{×} \frac{∂z_{2}}{∂W_{4}} → W_{4}^{+}=0.34928991$$

### 3) 결과확인

![](https://wikidocs.net/images/page/37406/nn1_final.PNG)

그럼 업데이트된 가중치를 이용하여 다시 순전파를 통해 나온 오차값이 이전 오차값에 비해 감소했는지 확인해보자. 

계산과정은 동일함으로 결과값만 비교한다.

### - 이전의 오차
$$E_{total}=E_{o1}+E_{o2}=0.02397190$$

### - 새로운 가중치를 이용한 오차
$$E_{total}=E_{o1}+E_{o2}=0.02323634$$

약 0.006의 오차가 감소함을 확인할 수 있다. 이렇게 인공신경망의 학습은 **오차를 최소화하는 가중치를 찾는 목적으로 순전파와 역전파를 반복하는 것을 의미한다.** 

## 참고자료
___

[https://wikidocs.net/24987](https://wikidocs.net/24987)
[https://creativecommons.org/licenses/by-nc-sa/2.0/kr/](https://creativecommons.org/licenses/by-nc-sa/2.0/kr/)




























